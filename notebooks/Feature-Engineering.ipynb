{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from utils import *\n",
    "from features import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(20,5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbit_rate = 1/125000\n",
    "\n",
    "low_fp = '../data/240p/' \n",
    "med_fp = '../data/480p/'\n",
    "high_fp = '../data/1080p/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdoan_low = pd.read_csv(low_fp + 'stdoan-101-action-240p-20201127.csv')\n",
    "stdoan_med = pd.read_csv(med_fp + 'stdoan-101-action-480p-20201127.csv')\n",
    "stdoan_high = pd.read_csv(high_fp + 'stdoan-101-action-1080p-20201127.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_ms = convert_ms_df(stdoan_low, True)\n",
    "med_ms = convert_ms_df(stdoan_med, True)\n",
    "high_ms = convert_ms_df(stdoan_high, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_resample = low_ms.resample('500ms', on='Time').sum()\n",
    "med_resample = med_ms.resample('500ms', on='Time').sum()\n",
    "high_resample = high_ms.resample('500ms', on='Time').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## take the aggregate features of the whole chunk; download and upload\n",
    "def agg_feat(df, col):\n",
    "  return [np.mean(df[col]), np.std(df[col])]\n",
    "\n",
    "## take the ratio of upload:download packets\n",
    "def pkt_ratio(df):\n",
    "  ms_df = convert_ms_df(df, True)\n",
    "  local = np.sum(ms_df['pkt_src'] == '1') \n",
    "  server = np.sum(ms_df['pkt_src'] == '2') \n",
    "  return local / server\n",
    "\n",
    "## take the ratio of upload:download bytes\n",
    "def bytes_ratio(df):\n",
    "  local = df['1->2Bytes'].sum()\n",
    "  server = df['2->1Bytes'].sum()\n",
    "  return local / server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imnemato-104-action-240p-20210110.csv',\n",
       " 'imnemato-105-still-240p-20210112.csv',\n",
       " 'saqian-102-action-240p-20210113.csv',\n",
       " 'sgs008-106-action-240p-20210112.csv',\n",
       " 'sgs008-107-still-240p-20210112.csv',\n",
       " 'shs214-103-still-240p-20210109.csv',\n",
       " 'shs214-108-action-240p-20210112.csv',\n",
       " 'stdoan-101-action-240p-20201127.csv',\n",
       " 'stdoan-102-action-240p-20201206.csv']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(low_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 300 bytes = small pkt\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak Related Aggregate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finds the peaks with mean + 2(1) std\n",
    "## run the above aggregate functions on the peaks only??\n",
    "\n",
    "def get_peak_loc(df, col, invert=False):\n",
    "  'invert arg allows you to get values not considered peaks'\n",
    "  df_avg = df[col].mean()\n",
    "  df_std = df[col].std()\n",
    "  \n",
    "  threshold = df_avg + (1 * df_std)\n",
    "  \n",
    "  if invert:\n",
    "    return np.array(df[col] < threshold)\n",
    "  \n",
    "  else:\n",
    "    return np.array(df[col] > threshold)\n",
    "\n",
    "## np.mean, np.var, np.std - think of more?  \n",
    "def peak_time_diff(df, col):\n",
    "  '''\n",
    "  mess around with the different inputs for function. \n",
    "  variance seems to inflate the difference betweent the two the most with litte\n",
    "  to no data manipulation. however, currently trying things like\n",
    "  squaring the data before taking the aggregate function to exaggerate\n",
    "  differences (moderate success??)\n",
    "  '''\n",
    "  peaks = df[get_peak_loc(df, col)]\n",
    "  peaks['Time'] = peaks['Time'] - peaks['Time'].min()\n",
    "  time_diff = np.diff(peaks['Time'] ** 2)\n",
    "  return [np.mean(time_diff), np.var(time_diff), np.std(time_diff)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_features(df, col):\n",
    "\n",
    "    \"\"\"\n",
    "    welch implemention of spectral features\n",
    "    resample the data before inputting (might change prereq depending on\n",
    "    resource allocation)\n",
    "    \"\"\"\n",
    "\n",
    "    f, Pxx_den = sp.signal.welch(df[col], fs=2)\n",
    "    Pxx_den = np.sqrt(Pxx_den)\n",
    "\n",
    "    peaks = sp.signal.find_peaks(Pxx_den)[0]\n",
    "    prominences = sp.signal.peak_prominences(Pxx_den, peaks)[0]\n",
    "\n",
    "    idx_max = prominences.argmax()\n",
    "    loc_max = peaks[idx_max]\n",
    "\n",
    "    return [f[loc_max], Pxx_den[loc_max], prominences[idx_max]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking & Feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## wip; need to decide chunk size eventually\n",
    "## should we also make this chunking feature be our feature creation?\n",
    "\n",
    "def chunk_data(df, interval=60):\n",
    "\n",
    "    \"\"\"\n",
    "    takes in a filepath to the data you want to chunk and feature engineer\n",
    "    chunks our data into a specified time interval\n",
    "    each chunk is then turned into an observation to be fed into our classifier\n",
    "    \"\"\"\n",
    "\n",
    "    df_list = []\n",
    "    \n",
    "    df['Time'] = df['Time'] - df['Time'].min()\n",
    "    \n",
    "    total_chunks = np.floor(df['Time'].max() / interval).astype(int)\n",
    "\n",
    "    for chunk in np.arange(total_chunks):\n",
    "      \n",
    "        start = chunk * interval\n",
    "        end = (chunk+1) * interval\n",
    "\n",
    "        temp_df = (df[(df['Time'] >= start) & (df['Time'] < end)])\n",
    "        \n",
    "        df_list.append(temp_df)\n",
    "        \n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, interval=60):\n",
    "\n",
    "  features = [\n",
    "    'dwl_peak_freq',\n",
    "    'dwl_peak_prom',\n",
    "    'dwl_max_psd',\n",
    "    'dwl_bytes_avg',\n",
    "    'dwl_bytes_std',\n",
    "    'dwl_peak_avg',\n",
    "    'dwl_peak_var',\n",
    "    'dwl_peak_std',\n",
    "    'upl_peak_freq',\n",
    "    'upl_peak_prom',\n",
    "    'upl_max_psd',\n",
    "    'upl_bytes_avg',\n",
    "    'upl_bytes_std',\n",
    "    'upl_peak_avg',\n",
    "    'upl_peak_var',\n",
    "    'upl_peak_std'\n",
    "  ]  \n",
    "\n",
    "  vals = []\n",
    "\n",
    "  df_chunks = chunk_data(df, interval)\n",
    "\n",
    "  for chunk in df_chunks:\n",
    "\n",
    "    preproc = convert_ms_df(chunk, True)\n",
    "    upl_bytes = preproc[preproc['pkt_src'] == '1'].resample('500ms', on='Time').sum()\n",
    "    dwl_bytes = preproc[preproc['pkt_src'] == '2'].resample('500ms', on='Time').sum()\n",
    "\n",
    "    ## spectral features\n",
    "    dwl_spectral = spectral_features(dwl_bytes, 'pkt_size')\n",
    "    upl_spectral = spectral_features(upl_bytes, 'pkt_size')\n",
    "    \n",
    "    ## aggregate features\n",
    "    dwl_agg = agg_feat(chunk, '2->1Bytes')\n",
    "    upl_agg = agg_feat(chunk, '1->2Bytes')\n",
    "    \n",
    "    ## peak features\n",
    "    dwl_peak = peak_time_diff(chunk, '2->1Bytes')\n",
    "    upl_peak = peak_time_diff(chunk, '1->2Bytes')\n",
    "    \n",
    "    feat_val = np.hstack((\n",
    "      dwl_spectral,\n",
    "      dwl_agg,\n",
    "      dwl_peak,\n",
    "      upl_spectral,\n",
    "      upl_agg,\n",
    "      upl_peak\n",
    "    ))\n",
    "    \n",
    "    vals.append(feat_val)\n",
    "    \n",
    "  return pd.DataFrame(columns=features, data=vals).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_no_split(df, interval=60):\n",
    "\n",
    "  features = [\n",
    "    'peak_freq',\n",
    "    'peak_prom',\n",
    "    'max_psd',\n",
    "    'bytes_avg',\n",
    "    'bytes_std',\n",
    "    'peak_avg',\n",
    "    'peak_var',\n",
    "    'peak_std',\n",
    "  ]  \n",
    "\n",
    "  vals = []\n",
    "\n",
    "  df_chunks = chunk_data(df, interval)\n",
    "\n",
    "  for chunk in df_chunks:\n",
    "\n",
    "    preproc = convert_ms_df(chunk, True)\n",
    "\n",
    "    ## spectral features\n",
    "    spectral_feat = spectral_features(preproc, 'pkt_size')\n",
    "    \n",
    "    ## aggregate features\n",
    "    aggr_feat = agg_feat(chunk, '2->1Bytes')\n",
    "    \n",
    "    ## peak features\n",
    "    peak_feat = peak_time_diff(chunk, '2->1Bytes')\n",
    "    \n",
    "    feat_val = np.hstack((\n",
    "      spectral_feat,\n",
    "      aggr_feat,\n",
    "      peak_feat\n",
    "    ))\n",
    "    \n",
    "    vals.append(feat_val)\n",
    "    \n",
    "  return pd.DataFrame(columns=features, data=vals).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_feat_no_split = create_features_no_split(stdoan_low, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "low_feat = create_features(stdoan_low, 100)\n",
    "med_feat = create_features(stdoan_med, 100)\n",
    "high_feat = create_features(stdoan_high, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_feat['resolution'] = np.zeros(len(low_feat))\n",
    "med_feat['resolution'] = np.zeros(len(med_feat)) + 1\n",
    "high_feat['resolution'] = np.zeros(len(high_feat)) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_split = pd.concat([low_feat, med_feat, high_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = training_split.drop(columns=['resolution']), training_split['resolution']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "                       max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=5,\n",
       "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_split = RandomForestClassifier(n_estimators = 5, max_depth = 2, criterion = 'entropy', random_state = 42)\n",
    "clf_split.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_split.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resolution       1.000000\n",
       "dwl_bytes_std    0.952209\n",
       "upl_bytes_std    0.941935\n",
       "dwl_max_psd      0.909083\n",
       "upl_max_psd      0.908157\n",
       "dwl_bytes_avg    0.900471\n",
       "upl_peak_prom    0.900044\n",
       "dwl_peak_prom    0.900041\n",
       "upl_bytes_avg    0.889333\n",
       "upl_peak_avg     0.517063\n",
       "dwl_peak_avg     0.503176\n",
       "upl_peak_std     0.498807\n",
       "upl_peak_var     0.477132\n",
       "dwl_peak_std     0.473051\n",
       "dwl_peak_var     0.450026\n",
       "dwl_peak_freq    0.442399\n",
       "upl_peak_freq    0.261256\n",
       "Name: resolution, dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(training_split.corr()['resolution']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted Resolution</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Resolution</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted Resolution  0.0  1.0  2.0\n",
       "Actual Resolution                  \n",
       "0.0                     5    0    0\n",
       "1.0                     0   10    0\n",
       "2.0                     0    1    3"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.crosstab(y_test, y_pred, rownames=['Actual Resolution'], colnames=['Predicted Resolution']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "low_feat_no_split = create_features_no_split(stdoan_low, 100)\n",
    "med_feat_no_split = create_features_no_split(stdoan_med, 100)\n",
    "high_feat_no_split = create_features_no_split(stdoan_high, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_feat_no_split['resolution'] = np.zeros(len(low_feat))\n",
    "med_feat_no_split['resolution'] = np.zeros(len(med_feat)) + 1\n",
    "high_feat_no_split['resolution'] = np.zeros(len(high_feat)) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_no_split = pd.concat([low_feat_no_split, med_feat_no_split, high_feat_no_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = training_no_split.drop(columns=['resolution']), training_no_split['resolution']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "                       max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=2,\n",
       "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_no_split = RandomForestClassifier(n_estimators = 2, max_depth = 2, criterion = 'entropy', random_state = 42)\n",
    "clf_no_split.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_no_split.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resolution    1.000000\n",
       "bytes_std     0.952209\n",
       "bytes_avg     0.900471\n",
       "max_psd       0.659674\n",
       "peak_avg      0.503176\n",
       "peak_std      0.473051\n",
       "peak_var      0.450026\n",
       "peak_freq     0.352513\n",
       "peak_prom     0.286279\n",
       "Name: resolution, dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(training_no_split.corr()['resolution']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted Resolution</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Resolution</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted Resolution  0.0  1.0  2.0\n",
       "Actual Resolution                  \n",
       "0.0                     4    1    0\n",
       "1.0                     6    4    0\n",
       "2.0                     1    2    1"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.crosstab(y_test, y_pred, rownames=['Actual Resolution'], colnames=['Predicted Resolution']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.47058824, 0.4       ])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
